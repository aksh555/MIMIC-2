{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D,Dense\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "test_val = np.load('test_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_val = np.load('y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37065, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(x_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Dropout, Activation, SpatialDropout1D, Reshape, GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamming_loss(y_true, y_pred):\n",
    "    tmp = K.abs(y_true-y_pred)\n",
    "    return K.mean(K.cast(K.greater(tmp,0.5),dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squash_function(Layer):\n",
    "    def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "        with tf.name_scope(name, default_name=\"squash\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keep_dims=True)\n",
    "            safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = s / safe_norm\n",
    "            return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1011)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1011, 20)     2227920     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 1011, 32)     5088        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (None, 1011, 32)     5088        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1011, 64)     0           gru_3[0][0]                      \n",
      "                                                                 gru_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "squash_function_2 (squash_funct (None, 1011, 64)     0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 1009, 32)     6176        squash_function_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1009, 32)     6176        squash_function_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1009, 64)     0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1009, 32)     6176        squash_function_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1009, 96)     0           concatenate_7[0][0]              \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_2 (A (None, 96)           96          concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 192)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_weighted_average_2[0][0\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 288)          0           concatenate_9[0][0]              \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           5780        concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,262,500\n",
      "Trainable params: 2,262,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPooling1D, Dropout, Conv1D,GlobalAveragePooling1D,Bidirectional,GRU,concatenate,Input\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "filter_length = 128\n",
    "num_classes = 20\n",
    "max_words = 111396\n",
    "maxlen = 1011\n",
    "\n",
    "input1 = Input(shape=(maxlen,))\n",
    " \n",
    "x = Embedding(max_words, 20, input_length=maxlen)(input1)\n",
    "\n",
    "\n",
    "gru1 = GRU(32, return_sequences=True, dropout=0.1,\n",
    "                                                      recurrent_dropout=0.1)(x)\n",
    "gru2 = GRU(32, return_sequences=True, dropout=0.1,\n",
    "                                                      recurrent_dropout=0.1)(x)\n",
    "x = concatenate([gru1,gru2])\n",
    "\n",
    "x = squash_function()(x)\n",
    " \n",
    "conv_64 = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    " \n",
    "conv1 = Conv1D(32, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "conv2 = Conv1D(32, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "conv3 = Conv1D(32, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "cat = concatenate([conv2,conv1])\n",
    "x = concatenate([cat,conv3])\n",
    "\n",
    "maxpool = GlobalMaxPooling1D()(x)\n",
    "attn = AttentionWeightedAverage()(x)\n",
    "avg = GlobalAveragePooling1D()(x)\n",
    "\n",
    "l = concatenate([maxpool,attn])\n",
    "x = concatenate([l,avg])\n",
    " \n",
    "preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    " \n",
    "model = keras.Model(input1, preds)\n",
    " \n",
    "model.summary()\n",
    " \n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Hamming_loss])\n",
    "\n",
    "\n",
    "# callbacks = [\n",
    "#     ReduceLROnPlateau(), \n",
    "#     EarlyStopping(patience=4)\n",
    "# ]\n",
    "\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     epochs=5,\n",
    "#                     batch_size=128,\n",
    "#                     validation_split=0.1,\n",
    "#                     callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "\n",
    "def _train_model(model, batch_size, train_x, train_y, test_val, y_val):\n",
    "    num_labels = train_y.shape[1]\n",
    "    patience = 7\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(test_val, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(num_labels):\n",
    "            loss = log_loss(y_val[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_labels\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == patience:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 1203s 32ms/step - loss: 0.4964 - Hamming_loss: 0.2467\n",
      "Epoch 0 loss 0.4361130736413029 best_loss -1\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 1198s 32ms/step - loss: 0.4230 - Hamming_loss: 0.2106\n",
      "Epoch 1 loss 0.41322811947662635 best_loss 0.4361130736413029\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 1201s 32ms/step - loss: 0.4067 - Hamming_loss: 0.1986\n",
      "Epoch 2 loss 0.4040020778238452 best_loss 0.41322811947662635\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 736s 20ms/step - loss: 0.3939 - Hamming_loss: 0.1889\n",
      "Epoch 3 loss 0.39399984504532376 best_loss 0.4040020778238452\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 360s 10ms/step - loss: 0.3842 - Hamming_loss: 0.1827\n",
      "Epoch 4 loss 0.38983792474951806 best_loss 0.39399984504532376\n",
      "Epoch 1/1\n",
      "35840/37065 [============================>.] - ETA: 11s - loss: 0.3756 - Hamming_loss: 0.1766"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "history = _train_model(model,batch_size,x_train,y_train,test_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = history.predict(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_pred[35],y_val[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(7.8654468e-01+           \n",
    "5.0963247e-01+\n",
    "9.6992075e-01+\n",
    "5.8361495e-01+\n",
    "3.5057139e-01+\n",
    "5.7766867e-01)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.where(y_pred>0.5,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score,roc_auc_score,confusion_matrix,hamming_loss\n",
    "print(jaccard_score(y_test,y_pred_round,average='micro'))\n",
    "print(jaccard_score(y_test,y_pred_round,average='macro'))\n",
    "print(roc_auc_score(y_test,y_pred_round))\n",
    "print(hamming_loss(y_test,y_pred_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('conv_n_gru.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mimic)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
