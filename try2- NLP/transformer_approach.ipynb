{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "test_val = np.load('test_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_val = np.load('y_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37065, 1011)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, InputSpec, add\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, activations\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProdSelfAttention(Layer):\n",
    "    \"\"\"The self-attention layer as in 'Attention is all you need'.\n",
    "    paper reference: https://arxiv.org/abs/1706.03762\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, units,\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(DotProdSelfAttention, self).__init__(*kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        input_dim = input_shape[-1]\n",
    "        # We assume the output-dim of Q, K, V are the same\n",
    "        self.kernels = dict.fromkeys(['Q', 'K', 'V'])\n",
    "        for key, _ in self.kernels.items():\n",
    "            self.kernels[key] = self.add_weight(shape=(input_dim, self.units),\n",
    "                                                initializer=self.kernel_initializer,\n",
    "                                                name='kernel_{}'.format(key),\n",
    "                                                regularizer=self.kernel_regularizer,\n",
    "                                                constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            raise NotImplementedError\n",
    "        super(DotProdSelfAttention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        Q = K.dot(x, self.kernels['Q'])\n",
    "        K_mat = K.dot(x, self.kernels['K'])\n",
    "        V = K.dot(x, self.kernels['V'])\n",
    "        attention = K.batch_dot(Q, K.permute_dimensions(K_mat, [0, 2, 1]))\n",
    "        d_k = K.constant(self.units, dtype=K.floatx())\n",
    "        attention = attention / K.sqrt(d_k)\n",
    "        attention = K.batch_dot(K.softmax(attention, axis=-1), V)\n",
    "        return attention\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squash_function(Layer):\n",
    "    def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "        with tf.name_scope(name, default_name=\"squash\"):\n",
    "            squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                         keep_dims=True)\n",
    "            safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "            squash_factor = squared_norm / (1. + squared_norm)\n",
    "            unit_vector = s / safe_norm\n",
    "            return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_tensor):\n",
    "    \n",
    "\n",
    "    \"\"\"One encoder as in Attention Is All You Need\"\"\"\n",
    "\n",
    "    # Sub-layer 1\n",
    "    # Multi-Head Attention\n",
    "    multiheads = []\n",
    "    d_v = embed_size // n_heads\n",
    "    for i in range(n_heads):\n",
    "        multiheads.append(DotProdSelfAttention(d_v)(input_tensor))\n",
    "    multiheads = concatenate(multiheads, axis=-1)\n",
    "    #multiheads = Conv1D(64, kernel_size=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(multiheads)\n",
    "    multiheads = Dense(embed_size)(multiheads)\n",
    "    #multiheads = Dropout(0.1)(multiheads)\n",
    "    \n",
    "    # Residual Connection\n",
    "    res_con = add([input_tensor, multiheads])\n",
    "    # Didn't use layer normalization, use Batch Normalization instead here\n",
    "    res_con = squash_function()(res_con)\n",
    "    \n",
    "    # Sub-layer 2\n",
    "    # 2 Feed forward layer\n",
    "    ff1 = Dense(32, activation='relu')(res_con)\n",
    "    ff2 = Dense(embed_size)(ff1)\n",
    "    output = add([res_con, ff2])\n",
    "    output = squash_function()(output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Embedding, Lambda, Dropout, Activation, SpatialDropout1D, Reshape, GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamming_loss(y_true, y_pred):\n",
    "    tmp = K.abs(y_true-y_pred)\n",
    "    return K.mean(K.cast(K.greater(tmp,0.5),dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1011)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1011, 20)     2227920     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_1 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_2 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_3 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_4 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_5 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_6 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_7 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_prod_self_attention_8 (DotP (None, 1011, 2)      120         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1011, 16)     0           dot_prod_self_attention_1[0][0]  \n",
      "                                                                 dot_prod_self_attention_2[0][0]  \n",
      "                                                                 dot_prod_self_attention_3[0][0]  \n",
      "                                                                 dot_prod_self_attention_4[0][0]  \n",
      "                                                                 dot_prod_self_attention_5[0][0]  \n",
      "                                                                 dot_prod_self_attention_6[0][0]  \n",
      "                                                                 dot_prod_self_attention_7[0][0]  \n",
      "                                                                 dot_prod_self_attention_8[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1011, 20)     340         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1011, 20)     0           embedding_1[0][0]                \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "squash_function_1 (squash_funct (None, 1011, 20)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1011, 32)     672         squash_function_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1011, 20)     660         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1011, 20)     0           squash_function_1[0][0]          \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "squash_function_2 (squash_funct (None, 1011, 20)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1010, 128)    5248        squash_function_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "squash_function_3 (squash_funct (None, 256)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           8224        squash_function_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           660         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,244,684\n",
      "Trainable params: 2,244,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "filter_length = 128\n",
    "num_classes = 20\n",
    "max_words = 111396\n",
    "maxlen = 1011\n",
    "embed_size = 20\n",
    "n_heads = 8\n",
    "from sklearn.metrics import hamming_loss as hamming\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "input1 = Input(shape=(maxlen,))\n",
    "x = Embedding(max_words, 20, input_length=maxlen)(input1)\n",
    "\n",
    "#x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "# Add positional encoding\n",
    "#x = AddPositionalEncoding()(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "for i in range(1):\n",
    "    x = encoder(x)\n",
    "# These are my own experiments\n",
    "x = Conv1D(128, kernel_size=2, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "#conc = AttentionWeightedAverage()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "#conc = concatenate([conc,attn])\n",
    "conc = squash_function()(conc)\n",
    "conc = Dense(32, activation=\"relu\")(conc)\n",
    "#conc = Dropout(0.1)(conc)\n",
    "\n",
    "preds = Dense(num_classes, activation=\"sigmoid\")(conc)\n",
    " \n",
    "model = keras.Model(input1, preds)\n",
    " \n",
    "model.summary()\n",
    " \n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Hamming_loss])\n",
    "\n",
    "# import tensorflow as tf\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "# session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# model.fit(x_train, y_train, batch_size=8, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "\n",
    "def _train_model(model, batch_size, train_x, train_y, test_val, y_val):\n",
    "    num_labels = train_y.shape[1]\n",
    "    patience = 7\n",
    "    best_loss = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    current_epoch = 0\n",
    "    \n",
    "    while True:\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
    "        y_pred = model.predict(test_val, batch_size=batch_size)\n",
    "\n",
    "        total_loss = 0\n",
    "        for j in range(num_labels):\n",
    "            loss = log_loss(y_val[:, j], y_pred[:, j])\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= num_labels\n",
    "\n",
    "        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n",
    "\n",
    "        current_epoch += 1\n",
    "        if total_loss < best_loss or best_loss == -1:\n",
    "            best_loss = total_loss\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == patience:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 255s 7ms/step - loss: 0.4146 - Hamming_loss: 0.2019\n",
      "Epoch 0 loss 0.38578275379417015 best_loss -1\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 299s 8ms/step - loss: 0.3728 - Hamming_loss: 0.1749\n",
      "Epoch 1 loss 0.3731546902232906 best_loss 0.38578275379417015\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 300s 8ms/step - loss: 0.3570 - Hamming_loss: 0.1659\n",
      "Epoch 2 loss 0.3706308835158708 best_loss 0.3731546902232906\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 299s 8ms/step - loss: 0.3441 - Hamming_loss: 0.1585\n",
      "Epoch 3 loss 0.3721834573695413 best_loss 0.3706308835158708\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 299s 8ms/step - loss: 0.3311 - Hamming_loss: 0.1511\n",
      "Epoch 4 loss 0.37841574437145165 best_loss 0.3706308835158708\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 300s 8ms/step - loss: 0.3173 - Hamming_loss: 0.1438\n",
      "Epoch 5 loss 0.3865556740172814 best_loss 0.3706308835158708\n",
      "Epoch 1/1\n",
      "37065/37065 [==============================] - 299s 8ms/step - loss: 0.3023 - Hamming_loss: 0.1359\n",
      "Epoch 6 loss nan best_loss 0.3706308835158708\n",
      "Epoch 1/1\n",
      "   16/37065 [..............................] - ETA: 5:17 - loss: 0.2466 - Hamming_loss: 0.0875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37065/37065 [==============================] - 300s 8ms/step - loss: 0.2874 - Hamming_loss: 0.1277\n",
      "Epoch 7 loss nan best_loss 0.3706308835158708\n",
      "Epoch 1/1\n",
      "   16/37065 [..............................] - ETA: 5:02 - loss: 0.2107 - Hamming_loss: 0.0688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37065/37065 [==============================] - 300s 8ms/step - loss: 0.2713 - Hamming_loss: 0.1198\n",
      "Epoch 8 loss nan best_loss 0.3706308835158708\n",
      "Epoch 1/1\n",
      "   16/37065 [..............................] - ETA: 5:38 - loss: 0.2631 - Hamming_loss: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37065/37065 [==============================] - 300s 8ms/step - loss: 0.2550 - Hamming_loss: 0.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss nan best_loss 0.3706308835158708\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "batch_size = 8\n",
    "history = _train_model(model,batch_size,x_train,y_train,test_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = history.predict(x_test, batch_size=batch_size)\n",
    "y_pred2 = history.predict(test_val, batch_size=batch_size)\n",
    "y_pred1=pd.DataFrame(y_pred1)\n",
    "y_pred2=pd.DataFrame(y_pred2)\n",
    "y_pred1.to_pickle(\"y_pred_transformer_approach3_test.pkl\")\n",
    "y_pred2.to_pickle(\"y_pred_transformer_approach3_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_round = np.where(y_pred1>0.5,1,0)\n",
    "y_pred_round1 = np.where(y_pred2>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5163860440895678\n",
      "0.3997940254773026\n",
      "0.6778189264605493\n",
      "0.17036199095022625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score,roc_auc_score,confusion_matrix,hamming_loss\n",
    "print(jaccard_score(y_test,y_pred_round,average='micro'))\n",
    "print(jaccard_score(y_test,y_pred_round,average='macro'))\n",
    "print(roc_auc_score(y_test,y_pred_round))\n",
    "print(hamming_loss(y_test,y_pred_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('transformer_approach6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshara/anaconda3/envs/mimic/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"./str+rad_text-combined.pkl\")\n",
    "df= df.drop([\"SUBJECT_ID\",\"text\",\"HADM_ID\"],axis=1)\n",
    "x = df.iloc[:,:483]\n",
    "y = df.iloc[:,483:]\n",
    "from sklearn.model_selection import train_test_split\n",
    "#train,test,val \n",
    "seed = 40\n",
    "x_tra,x_test,y_tra,y_test = train_test_split(x,y,test_size=0.15,random_state=seed)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_tra,y_tra,test_size = 0.2,random_state=seed)\n",
    "\n",
    "\n",
    "%run helper_functions.ipynb\n",
    "\n",
    "x_train_txt = np.load('x_train.npy') \n",
    "x_test_txt = np.load('x_test.npy')\n",
    "test_val_txt = np.load('test_val.npy') \n",
    "y_test_txt = np.load('y_test.npy')\n",
    "y_train_txt = np.load('y_train.npy')\n",
    "y_val_txt = np.load('y_val.npy')\n",
    "batch_size1 = 64\n",
    "\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "conv1 = load_model('conv1.h5',custom_objects={'squash_function': squash_function,\n",
    "                                              'AttentionWeightedAverage':AttentionWeightedAverage,\n",
    "                                              'Hamming_loss':Hamming_loss\n",
    "                                             })\n",
    "\n",
    "conv3 = load_model('conv3.h5',custom_objects={'squash_function': squash_function,\n",
    "                                              'AttentionWeightedAverage':AttentionWeightedAverage,\n",
    "                                              'Hamming_loss':Hamming_loss\n",
    "                                             })\n",
    "\n",
    "y_pred_conv3 = conv3.predict(test_val_txt, batch_size=batch_size1)\n",
    "\n",
    "y_pred_conv1 = conv1.predict(test_val_txt, batch_size=batch_size1)\n",
    "\n",
    "\n",
    "print(y_pred_conv3.shape,y_pred.shape,y_pred_lgb.shape)\n",
    "\n",
    "# Structured models\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "cat = joblib.load(\"/home/akshara/MIMIC-Project/Trained Models/CatBoostC.pkl\")\n",
    "lgb = joblib.load(\"/home/akshara/MIMIC-Project/try1/LGBM_40.pkl\")\n",
    "\n",
    "y_pred_cat = cat.predict(x_val)\n",
    "y_pred_lgb = lgb.predict(x_val)\n",
    "\n",
    "def corr(a, b):\n",
    "    counts = 0\n",
    "    for i in range(a.shape[0]):  \n",
    "      # correlation\n",
    "        print(\"Finding correlation row : {}\".format(i))\n",
    "      #print(\"Column to be measured: {}\".format(prediction))\n",
    "        cor = np.corrcoef(a[i,:],b[i,:])\n",
    "        print(\"Pearson's correlation score: {}\".format(cor))\n",
    "        if(cor[0][1] < 0.8):  \n",
    "            counts+=1\n",
    "    return counts\n",
    "\n",
    "\n",
    "#l = [corr(y_pred_conv3, y_pred_conv1),corr(y_pred_conv3, y_pred_cat),corr(y_pred_conv1, y_pred_cat),corr(y_pred_conv3, y_pred_lgb),corr(y_pred_conv1, y_pred_lgb)] \n",
    "l = [corr(y_pred_conv3, y_pred),corr(y_pred, y_pred_lgb),corr(y_pred_conv1, y_pred)] \n",
    "\n",
    "l\n",
    "\n",
    "y_pred1 =0.6*y_pred+0.3*y_pred_lgb+0.1*y_pred_conv3\n",
    "y_pred_round = np.where(y_pred1>0.5,1,0)\n",
    "\n",
    "#y_pred_round = np.where(y_pred>0.5,1,0)\n",
    "from sklearn.metrics import jaccard_score,roc_auc_score,confusion_matrix,hamming_loss\n",
    "print(jaccard_score(y_val,y_pred_round,average='micro'))\n",
    "print(jaccard_score(y_val,y_pred_round,average='macro'))\n",
    "print(roc_auc_score(y_val,y_pred_round))\n",
    "print(hamming_loss(y_val,y_pred_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mimic)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
